{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 目标：构建具有“记忆”的聊天机器人\n",
    "# 对话中模型具有上下文记忆，多用户对话\n",
    "# 对话历史记录管理（消息过长裁剪、保留system消息）\n",
    "# 对话信息持久化"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# 相关环境变量设置\n",
    "import config_loader\n",
    "\n",
    "config_loader.load_env()"
   ],
   "id": "b7841fce372e1a82",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 我们单次调用模型聊天，它是不会记住上次聊了什么的\n",
    "# 我们想让他知道之前的对话记录，就需要把之前的对话记录发给他，然后附上最新的问题\n",
    "# 这样就让模型有了“记忆”的效果\n",
    "# 参考如下示例\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
    "ai_msg = llm.invoke(\"我的名字是张三\")\n",
    "print(ai_msg.content)\n",
    "\n",
    "print(\"=\"*100)\n",
    "ai_msg = llm.invoke(\"我是谁？\")\n",
    "# 并不会记住我的名字\n",
    "print(ai_msg.content)"
   ],
   "id": "f54c0e64c25e413",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 我们需要把之前的对话记录全部都发送给LLM 就会有记忆效果\n",
    "from langchain_core.messages.human import HumanMessage\n",
    "from langchain_core.messages.ai import AIMessage\n",
    "\n",
    "\n",
    "msg_list = [\n",
    "    HumanMessage(content=\"我的名字是张三\"),\n",
    "    AIMessage(content=\"你好，张三！很高兴认识你。有什么我可以帮你的吗？\"),\n",
    "    HumanMessage(content=\"我是谁？\")\n",
    "]\n",
    "llm.invoke(msg_list).content"
   ],
   "id": "5dc80846174706ed",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 上面是模型记忆的原理，我们需要持久化对话记录，自动封装之前的上下文\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "\n",
    "# 定义一个 graph\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "# 定义一个调用模型的函数\n",
    "def call_model(state: MessagesState):\n",
    "    # 可以观察这行日志，lang graph 会自动拼接之前的对话记录封装到 MessageState 对象中\n",
    "    print(\"当前 MessageState: \", state)\n",
    "    print(\"当前 messages: \", state[\"messages\"])\n",
    "    response = llm.invoke(state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "# 定义 graph 中的（单个）节点\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "# 添加记忆存储 这个支持内存、sqlite、postgres，参考：https://langchain-ai.github.io/langgraph/concepts/persistence/#checkpointer-libraries\n",
    "# 默认是内存存储数据，生产环境推荐 postgres\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)\n",
    "\n",
    "# 配置当前用户线程ID 这样单个app 就能支持多个对话线程\n",
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "\n",
    "query = \"你好，我是张三\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()\n"
   ],
   "id": "d98887f15701351e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "query = \"我叫什么名字？\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ],
   "id": "c522231dd13909ac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 换个对话线程id 就不会把其他对话线程id给带出来\n",
    "config = {\"configurable\": {\"thread_id\": \"aaa\"}}\n",
    "\n",
    "query = \"我叫什么名字？\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ],
   "id": "5a936820da6242f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 但是还是用原来的线程id 就还保留着之前的记忆\n",
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "\n",
    "query = \"我叫什么名字？\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ],
   "id": "c28fd5006e2536b5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 添加提示词模板\n",
    "# 上面介绍了对话如何带有上下文记忆，现在介绍对话的时候如何带有提示词模板\n",
    "# 比如说现在需要预输入一个系统提示词，然后并带有“语言”参数\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages.system import SystemMessage\n",
    "\n",
    "# MessagesPlaceholder 传递所有消息\n",
    "# 预输入一个系统提示词\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content=\"你是一名老中医，尽你最大的能力用 {language} 去回答所有问题\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ],
   "id": "29ddff3d32aa3021",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from typing import Sequence\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "\n",
    "# 因为我们还有个 language 参数，所以我们需要自定义state约束\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    # 定义 language 属性和属性类型\n",
    "    language: str\n",
    "\n",
    "workflow = StateGraph(state_schema=State)\n",
    "\n",
    "def call_model(state: State):\n",
    "    # 这里对 state 进行模板转换\n",
    "    prompt = prompt_template.invoke(state)\n",
    "    print(\"当前 prompt: \", prompt)\n",
    "    response = llm.invoke(prompt)\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ],
   "id": "8015e82c8ef9bd14",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"test-with-prompt\"}}\n",
    "query = \"Hi! I'm zhangsan\"\n",
    "language = \"中文\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ],
   "id": "250a1e45c6857438",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 因为整个状态都是持久的，所以language参数没有变动的时候，可以不传参\n",
    "query = \"我是谁？\"\n",
    "input_messages = [HumanMessage(query)]\n",
    "\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ],
   "id": "fc9384594e8be2f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 新问题：大模型可接受的上下文长度终归是有限的，总不能把所有的对话历史记录都传递给它吧\n",
    "# 所以就有了限制传入消息的大小操作 这里用 trim_messages 来减少发送给模型的消息数量\n",
    "# 它允许我们指定要保留多少个标记，例如始终保留system消息和允许部分human消息\n",
    "from langchain_core.messages import SystemMessage, trim_messages\n",
    "\n",
    "trimmer = trim_messages(\n",
    "    # 限制消息的最大token长度\n",
    "    max_tokens=65,\n",
    "    strategy=\"last\",\n",
    "    token_counter=llm,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"你是一名老中医，专治吹牛*\"),\n",
    "    HumanMessage(content=\"你好，我是张三，以前和秦始皇掰过手腕\"),\n",
    "    AIMessage(content=\"我信你\"),\n",
    "    HumanMessage(content=\"我上周去火星转了一圈，发现没有西兰花，所以我回来了\"),\n",
    "    AIMessage(content=\"厉害\"),\n",
    "    HumanMessage(content=\"你说 1 + 1 等于几？\"),\n",
    "    AIMessage(content=\"老夫认为，如果是合作共赢的话，两个人合作会出现1+1>2的效果！\"),\n",
    "    HumanMessage(content=\"我谢谢你\"),\n",
    "    AIMessage(content=\"包的\"),\n",
    "    HumanMessage(content=\"你快乐吗？\"),\n",
    "    AIMessage(content=\"快乐\"),\n",
    "]\n",
    "\n",
    "# 运行结果可以看到，根据配置，保留了system消息，和在最大token的限制下，移除了早期的聊天记录\n",
    "trimmer.invoke(messages)"
   ],
   "id": "83cd7af082b7341",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 实际使用这个，其实只需要在调用模型之前，invoke 一下，把messages裁剪一下就行\n",
    "workflow = StateGraph(state_schema=State)\n",
    "\n",
    "\n",
    "def call_model(state: State):\n",
    "    trimmed_messages = trimmer.invoke(state[\"messages\"])\n",
    "    print(\"裁剪后的messages: \", trimmed_messages)\n",
    "    prompt = prompt_template.invoke(\n",
    "        {\"messages\": trimmed_messages, \"language\": state[\"language\"]}\n",
    "    )\n",
    "    response = llm.invoke(prompt)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ],
   "id": "40b243247a376773",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"test-with-prompt\"}}\n",
    "query = \"我是谁？\"\n",
    "language = \"中文\"\n",
    "\n",
    "# 把之前超过token的messages测试数据也带上\n",
    "input_messages = messages + [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    ")\n",
    "# 根据输出信息，可以发现裁剪有效！\n",
    "output[\"messages\"][-1].pretty_print()"
   ],
   "id": "6e492f74d320be82",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 如何流式输出？\n",
    "# 直接调用 stream 方法即可 参数与 invoke 类似\n",
    "config = {\"configurable\": {\"thread_id\": \"test-with-prompt\"}}\n",
    "query = \"我是谁？\"\n",
    "language = \"中文\"\n",
    "\n",
    "# 把之前超过token的messages测试数据也带上\n",
    "input_messages = messages + [HumanMessage(query)]\n",
    "\n",
    "for chunk, metadata in app.stream(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    "    # 加上流式输出模式\n",
    "    stream_mode=\"messages\",\n",
    "):\n",
    "    if isinstance(chunk, AIMessage):\n",
    "        print(chunk.content, end=\"|\")"
   ],
   "id": "2593c6d3b368654e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "e326bb17dac4ef76",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
