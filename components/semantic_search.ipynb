{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 目标 构建语义搜索引擎\n",
    "# 1.文档和文档加载\n",
    "# 2.文本拆分器使用\n",
    "# 3.嵌入模型使用\n",
    "# 4.向量存储和召回（就是根据语义检索相似文档块）"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "# 依赖安装\n",
    "%pip install langchain-community pypdf"
   ],
   "id": "ffa5105f178c5cb7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 相关环境变量设置\n",
    "import config_loader\n",
    "\n",
    "config_loader.load_env()"
   ],
   "id": "9ddf96af57381fd8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 文档和文档加载器\n",
    "# 文档（Document）是 langchain 的一个抽象概念，代表文本单元和相关元数据，有三个属性\n",
    "# page_content 文档内容字符串\n",
    "# metadata 包含任意元数据的字典，比如说获取相关文档来源，与其他文档的关系及其它额外信息\n",
    "# id 文档的字符串标识符\n",
    "# 一般单个 Document 对象代表较大文档的一部分\n",
    "# 文档使用示例\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=\"Dogs are great companions, known for their loyalty and friendliness.\",\n",
    "        metadata={\"source\": \"mammal-pets-doc\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Cats are independent pets that often enjoy their own space.\",\n",
    "        metadata={\"source\": \"mammal-pets-doc\"},\n",
    "    ),\n",
    "]\n",
    "documents"
   ],
   "id": "872be08b808a3b74",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 加载pdf文件为 Document 对象\n",
    "# 使用基于 pypdf 的 pdf 加载器 pdf一个页面会加载成为一个 Document 对象\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "file_path = \"../data/Kafka权威指南-22-52.pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "print(len(docs))\n",
    "print('='*100)\n",
    "# document 对象有原始文档字符串和元数据\n",
    "print(f\"{docs[0].page_content[:200]}\\n\")\n",
    "print(docs[0].metadata)"
   ],
   "id": "da264d37b0605c38",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 对于信息检索来说，一个页面一个 Document 太粗略了，所以需要进一步细致的拆分\n",
    "# 这里就用上了文本分割器 RecursiveCharacterTextSplitter，这个分割器是使用常用分隔符（比如说换行符）递归分割文档，直到每个块的大小合适\n",
    "# 这也是针对一般文本用例推荐的分割器\n",
    "# 我们将文档分割成1000个字符的块，块之间有200个字符的重叠，重叠有助于减轻将语句与与其相关的重要上下文分离的问题\n",
    "# 我们设置 add_start_index=True 每个分割文档在初始文档中开始的字符索引作为元数据属性 “start_index” 保存。\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
    ")\n",
    "# 基于上面的文档集合 再次通过文本拆分成更多的小文档块 返回一个 List[Document]\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(type(all_splits[0]))\n",
    "print(len(all_splits))\n",
    "print(\"=\"*100)\n",
    "print(f'第一个chunk：{all_splits[0]}')"
   ],
   "id": "dfdee1698c09d112",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 嵌入模型使用\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "# 初始化\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")\n",
    "\n",
    "# 使用嵌入模型 将文本块生成向量嵌入 List[float]\n",
    "vector_1 = embeddings.embed_query(all_splits[0].page_content)\n",
    "vector_2 = embeddings.embed_query(all_splits[1].page_content)\n",
    "\n",
    "assert len(vector_1) == len(vector_2)\n",
    "print(f\"生成的向量嵌入长度 {len(vector_1)}\\n\")\n",
    "print(vector_1[:10])"
   ],
   "id": "3107b134032a22ff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 向量存储 有多种方式可以内存，也可以用 FAISS 第三方向量库 这里方便起见 用内存\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "# 初始化向量存储\n",
    "vector_store = InMemoryVectorStore(embeddings)\n",
    "# 把文档做索引操作 加到库中\n",
    "ids = vector_store.add_documents(documents=all_splits)\n",
    "# 语义检索 返回语义相似的文档对象list\n",
    "results = vector_store.similarity_search(\n",
    "    \"Kafka Broker 配置\"\n",
    ")\n",
    "\n",
    "print(len(results))\n",
    "\n",
    "print(results)\n",
    "print(results[0])\n",
    "\n",
    "# 返回带分数的\n",
    "results = vector_store.similarity_search_with_score(\n",
    "    \"Kafka Broker 配置\"\n",
    ")\n",
    "doc, score = results[0]\n",
    "print(f\"Score: {score}\\n\")\n",
    "print(doc)"
   ],
   "id": "8a821a3e788bea74",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 根据向量嵌入来查询相似性\n",
    "query_embedding = embeddings.embed_query(\"Kafka Broker 配置\")\n",
    "results = vector_store.similarity_search_with_score_by_vector(query_embedding)\n",
    "\n",
    "print(len(results))\n",
    "doc, score = results[0]\n",
    "print(f\"Score: {score}\\n\")\n",
    "print(doc)"
   ],
   "id": "6b99fff0b896fb05",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
