{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 目标 构建语义搜索引擎\n",
    "# 1.文档和文档加载\n",
    "# 2.文本拆分器使用\n",
    "# 3.嵌入模型使用\n",
    "# 4.向量存储和召回（就是根据语义检索相似文档块）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffa5105f178c5cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mResolved \u001b[1m142 packages\u001b[0m \u001b[2min 1.38s\u001b[0m\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m numpy \u001b[2m(12.3MiB)\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m langchain-community \u001b[2m(2.4MiB)\u001b[0m\n",
      " \u001b[32m\u001b[1mDownloaded\u001b[0m\u001b[39m langchain-community\n",
      " \u001b[32m\u001b[1mDownloaded\u001b[0m\u001b[39m numpy\n",
      "\u001b[2mPrepared \u001b[1m17 packages\u001b[0m \u001b[2min 14.16s\u001b[0m\u001b[0m\n",
      "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mFailed to hardlink files; falling back to full copy. This may lead to degraded performance.\n",
      "         If the cache and target directories are on different filesystems, hardlinking may not be supported.\n",
      "         If this is intentional, set `export UV_LINK_MODE=copy` or use `--link-mode=copy` to suppress this warning.\u001b[0m\n",
      "\u001b[2mInstalled \u001b[1m17 packages\u001b[0m \u001b[2min 3.07s\u001b[0m\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1maiohappyeyeballs\u001b[0m\u001b[2m==2.6.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1maiohttp\u001b[0m\u001b[2m==3.11.14\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1maiosignal\u001b[0m\u001b[2m==1.3.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdataclasses-json\u001b[0m\u001b[2m==0.6.7\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfrozenlist\u001b[0m\u001b[2m==1.5.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhttpx-sse\u001b[0m\u001b[2m==0.4.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlangchain-community\u001b[0m\u001b[2m==0.3.20\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmarshmallow\u001b[0m\u001b[2m==3.26.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmultidict\u001b[0m\u001b[2m==6.2.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmypy-extensions\u001b[0m\u001b[2m==1.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==2.2.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpropcache\u001b[0m\u001b[2m==0.3.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpydantic-settings\u001b[0m\u001b[2m==2.8.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpypdf\u001b[0m\u001b[2m==5.4.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpython-dotenv\u001b[0m\u001b[2m==1.0.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtyping-inspect\u001b[0m\u001b[2m==0.9.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1myarl\u001b[0m\u001b[2m==1.18.3\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 依赖安装\n",
    "%uv add langchain-community pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddf96af57381fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 相关环境变量设置\n",
    "import sys \n",
    "sys.path.append(\"..\") \n",
    "from config import config_loader\n",
    "\n",
    "config_loader.load_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "872be08b808a3b74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'mammal-pets-doc'}, page_content='Dogs are great companions, known for their loyalty and friendliness.'),\n",
       " Document(metadata={'source': 'mammal-pets-doc'}, page_content='Cats are independent pets that often enjoy their own space.')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 文档和文档加载器\n",
    "# 文档（Document）是 langchain 的一个抽象概念，代表文本单元和相关元数据，有三个属性\n",
    "# page_content 文档内容字符串\n",
    "# metadata 包含任意元数据的字典，比如说获取相关文档来源，与其他文档的关系及其它额外信息\n",
    "# id 文档的字符串标识符\n",
    "# 一般单个 Document 对象代表较大文档的一部分\n",
    "# 文档使用示例\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=\"Dogs are great companions, known for their loyalty and friendliness.\",\n",
    "        metadata={\"source\": \"mammal-pets-doc\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Cats are independent pets that often enjoy their own space.\",\n",
    "        metadata={\"source\": \"mammal-pets-doc\"},\n",
    "    ),\n",
    "]\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da264d37b0605c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n",
      "====================================================================================================\n",
      "第1 章\n",
      "初识Kafka\n",
      "数据为企业的发展提供动力。我们从数据中获取信息，对它们进行分析处理，然后生成更\n",
      "多的数据。每个应用程序都会产生数据，包括日志消息、度量指标、用户活动记录、响应\n",
      "消息等。数据的点点滴滴都在暗示一些重要的事情，比如下一步行动的方向。我们把数据\n",
      "从源头移动到可以对它们进行分析处理的地方，然后把得到的结果应用到实际场景中，这\n",
      "样才能够确切地知道这些数据要告诉我们什么。例如，我们每\n",
      "\n",
      "{'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-03-22T07:33:14+00:00', 'source': '../data/Kafka权威指南-22-52.pdf', 'total_pages': 31, 'page': 0, 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "# 加载pdf文件为 Document 对象\n",
    "# 使用基于 pypdf 的 pdf 加载器 pdf一个页面会加载成为一个 Document 对象\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "file_path = \"../data/Kafka权威指南-22-52.pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "print(len(docs))\n",
    "print('='*100)\n",
    "# document 对象有原始文档字符串和元数据\n",
    "print(f\"{docs[0].page_content[:200]}\\n\")\n",
    "print(docs[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfdee1698c09d112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.documents.base.Document'>\n",
      "50\n",
      "====================================================================================================\n",
      "第一个chunk：page_content='第1 章\n",
      "初识Kafka\n",
      "数据为企业的发展提供动力。我们从数据中获取信息，对它们进行分析处理，然后生成更\n",
      "多的数据。每个应用程序都会产生数据，包括日志消息、度量指标、用户活动记录、响应\n",
      "消息等。数据的点点滴滴都在暗示一些重要的事情，比如下一步行动的方向。我们把数据\n",
      "从源头移动到可以对它们进行分析处理的地方，然后把得到的结果应用到实际场景中，这\n",
      "样才能够确切地知道这些数据要告诉我们什么。例如，我们每天在Amazon 网站上浏览感\n",
      "兴趣的商品，浏览信息被转化成商品推荐，并在稍后展示给我们。\n",
      "这个过程完成得越快，组织的反应就越敏捷。花费越少的精力在数据移动上，就越能专注\n",
      "于核心业务。这就是为什么在一个以数据为驱动的企业里，数据管道会成为关键性组件。\n",
      "如何移动数据，几乎变得与数据本身一样重要。\n",
      "每一次科学家们发生分歧，都是因为掌握的数据不够充分。所以我们可以先就获\n",
      "取哪一类数据达成一致。只要获取了数据，问题也就迎刃而解了。要么我是对\n",
      "的，要么你是对的，要么我们都是错的。然后我们继续研究。\n",
      "——Neil deGrasse Tyson\n",
      "1.1　发布与订阅消息系统\n",
      "在正式讨论 Apache Kafka（以下简称 Kafka）之前，先来了解发布与订阅消息系统的概念，\n",
      "并认识这个系统的重要性。数据（消息）的发送者（发布者）不会直接把消息发送给接收\n",
      "者，这是发布与订阅消息系统的一个特点。发布者以某种方式对消息进行分类，接收者\n",
      "（订阅者）订阅它们，以便接收特定类型的消息。发布与订阅系统一般会有一个 broker，也\n",
      "就是发布消息的中心点。\n",
      "1' metadata={'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-03-22T07:33:14+00:00', 'source': '../data/Kafka权威指南-22-52.pdf', 'total_pages': 31, 'page': 0, 'page_label': '1', 'start_index': 0}\n"
     ]
    }
   ],
   "source": [
    "# 对于信息检索来说，一个页面一个 Document 太粗略了，所以需要进一步细致的拆分\n",
    "# 这里就用上了文本分割器 RecursiveCharacterTextSplitter，这个分割器是使用常用分隔符（比如说换行符）递归分割文档，直到每个块的大小合适\n",
    "# 这也是针对一般文本用例推荐的分割器\n",
    "# 我们将文档分割成1000个字符的块，块之间有200个字符的重叠，重叠有助于减轻将语句与与其相关的重要上下文分离的问题\n",
    "# 我们设置 add_start_index=True 每个分割文档在初始文档中开始的字符索引作为元数据属性 “start_index” 保存。\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
    ")\n",
    "# 基于上面的文档集合 再次通过文本拆分成更多的小文档块 返回一个 List[Document]\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(type(all_splits[0]))\n",
    "print(len(all_splits))\n",
    "print(\"=\"*100)\n",
    "print(f'第一个chunk：{all_splits[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3107b134032a22ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成的向量嵌入长度 768\n",
      "\n",
      "[0.0535992830991745, 0.04645499959588051, -0.06358546018600464, 0.001913669635541737, 0.02215440571308136, 0.022496016696095467, 0.03912724554538727, -0.012047704309225082, -0.03164248913526535, -0.007910564541816711]\n"
     ]
    }
   ],
   "source": [
    "# 嵌入模型使用\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "# 初始化\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")\n",
    "\n",
    "# 使用嵌入模型 将文本块生成向量嵌入 List[float]\n",
    "vector_1 = embeddings.embed_query(all_splits[0].page_content)\n",
    "vector_2 = embeddings.embed_query(all_splits[1].page_content)\n",
    "\n",
    "assert len(vector_1) == len(vector_2)\n",
    "print(f\"生成的向量嵌入长度 {len(vector_1)}\\n\")\n",
    "print(vector_1[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a821a3e788bea74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "[Document(id='05086e2c-e5ae-4250-a9d5-36e3075bddb1', metadata={'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-03-22T07:33:14+00:00', 'source': '../data/Kafka权威指南-22-52.pdf', 'total_pages': 31, 'page': 24, 'page_label': '25', 'start_index': 0}, page_content='安装Kafka   ｜   25\\n生产者\\n生产者\\nKafka集群\\n主题A\\n主题A\\n主题B\\n分区0\\n分区1\\n分区0\\n消费者\\n图 2-2：一个简单的 Kafka 集群\\n2.6.1\\u3000需要多少个broker\\n一个 Kafka 集群需要多少个 broker 取决于以下几个因素。首先，需要多少磁盘空间来保\\n留数据，以及单个 broker 有多少空间 可用。如果整个集群需要保留 10TB 的数据，每个\\nbroker 可以存储 2TB，那么至少需要 5 个 broker。如果启用了数据复制，那么至少还需要\\n一倍的空间，不过这要取决于配置的复制系数是多少 （将在第 6 章介绍）\\n。也就是说，如\\n果启用了数据复制，那么这个集群至少需要 10 个 broker。\\n第二\\n个要考虑的因素是集群处理请求的能力。这通常与网络接口处理客户端流量的能力有\\n关，特别是当有多个消费者存在或者在数据保留期间流量发生波动（比如高峰时段的流量\\n爆发）时。如果单个 broker 的网络接口在 高峰时段可以达到 80% 的使用量，并且有两个\\n消费者，那么消费者就无法保持峰值，除非有两个 broker。如果集群启用了复制功能，则\\n要把这个额外的消费者考虑在内。因磁盘吞吐量低和系统内存不足造成的性能问题，也可\\n以通过扩展多个 broker 来解决。\\n2.6.2\\u3000broker 配置\\n要把一个 broker 加入到集群里，只需要修改两个配置参数。首先，所有 broker 都必须配\\n置相同的 zookeeper.connect，该参数指定了用于保存元数据的 Zookeeper 群组和路径。\\n其次，每个 broker 都必须为 broker.id 参数设置唯一的值。如果两个 broker 使用相同的\\nbroker.id，那么第二个 broker 就无法启动。在运行集群时，还可以配置其他一些参数，特\\n别是那些用于控制数据复制的参数，这些将在后续的章节介绍。'), Document(id='9c68861f-2e77-48cf-96bf-ce6efa12dfe7', metadata={'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-03-22T07:33:14+00:00', 'source': '../data/Kafka权威指南-22-52.pdf', 'total_pages': 31, 'page': 8, 'page_label': '9', 'start_index': 0}, page_content='初识Kafka   ｜   9\\n息流，并保证整个群组对每个给定的消息只处理一次。\\n1.3.3\\u3000基于磁盘的数据存储\\nKafka 不仅支持多个消费者，还允许消费者非实时地读取消息，这要归功于 Kafka 的数据\\n保留特性。消息被提交到磁盘，根据设置的保留规则进行保存。每个主题可以设置单独的\\n保留规则，以便满足不同消费者的需求，各个主题可以保留不同数量的消息。消费者可能\\n会因为处理速度慢或突发的流量高峰导致无法及时读取消息，而持久化数据可以保证数据\\n不会丢失。消费者可以在进行应用程序维护时离线一小段时间，而无需担心消息丢失或堵\\n塞在生产者端。消费者可以被关闭，但消息会继续保留在Kafka 里。消费者可以从上次中\\n断的地方继续处理消息。\\n1.3.4\\u3000伸缩性\\n为了能够轻松处理大量数据，Kafka 从一开始就被设计成一个具有灵活伸缩性的系统。用\\n户在开发阶段可以先使用单个 broker，再扩展到包含 3 个 broker 的小型开发集群，然后随\\n着数据量不断增长，部署到生产环境的集群可能包含上百个 broker。对在线集群进行扩展\\n丝毫\\n不影响整体系统的可用性。也就是说，一个包含多个 broker 的集群， 即使个别 broker\\n失效，仍然可以持续地为客户提供服务。要提高集群的容错能力，需要配置较高的复制系\\n数。第6 章将讨论关于复制的更多细节。\\n1.3.5\\u3000高性能\\n上面提到的所有特性，让 Kafka 成为了一个高性能的发布与订阅消息系统。通过横向扩展\\n生产者、消费者和 broker，Kafka 可以轻松 处理巨大的消息流。在处理大量数据的同时，\\n它还能保证亚秒级的消息延迟。\\n1.4\\u3000数据生态系统\\n已经有很多应用程序加入到了数据处理的大军中。我们定义了输入和应用程序，负责生成\\n数据或者把数据引入系统。我们定义了输出，它们可以是度量指标、报告或者其他类型的\\n数据。我们创建了一些循环，使用一些组件从系统读取数据，对读取的数据进行处理，然\\n后把它们导到数据基础设施上，以备不时之需。数据类型可以多种多样，每一种数据类型\\n可以有不同的内容、大小和用途。\\nKafka 为数据生态系统带来了循环系统，如图 1-9 所示。它在基础设施的各个组件之间传\\n递消息，为所有客户端提供一致的接口\\n。当与提供消息模式的系统集成时，生产者和消费'), Document(id='9d631f70-4f47-444a-bb4e-8f1ec677ca40', metadata={'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-03-22T07:33:14+00:00', 'source': '../data/Kafka权威指南-22-52.pdf', 'total_pages': 31, 'page': 5, 'page_label': '6', 'start_index': 0}, page_content='6   ｜   第 1 章\\n消费者读取消息。在其他发布与订阅系统中，消费者可能被称为 订阅者或读者。消费者订\\n阅一个或多个主题，并按照消息生成的顺序读取它们。消费者通过检查消息的偏移量来区\\n分已经读取过的消息。偏移量是另一种元数据，它是一个不断递增的整数值，在创建消息\\n时，Kafka 会把它添加到消息里。在给定的分区里，每个消息的偏移量都是唯一的 。消费\\n者把每个分区最后读取的消息偏移量保存在 Zookeeper 或 Kafka 上\\n，如果消费者关闭或重\\n启，它的读取状态不会丢失。\\n消费者是消费者群组的一部分，也就是说，会有一个或多个消费者共同读取一个主题\\n。群\\n组保证每个分区只能被一个消费者使用。图 1-6 所示的群组中，有 3 个消费者同时读取一\\n个主题。其中的两个消费者各自读取一个分区，另外一个消费者读取其他两个分区\\n。消费\\n者与分区之间的映射通常被称为消费者对分区的所有权关系。\\n通过这种方式，消费者可以消费包含大量消息的主题。而且，如果一个消费者失效，群组\\n里的其他消费者可以接管失效消费者的工作。第 4 章将详细介绍消费者和消费者群组。\\n分区0\\n分区1\\n分区2\\n分区3\\n主题“topicName”\\n消费者0\\n消费者1\\n消费者2\\n消费者\\n群组\\n图 1-6：消费者群组从主题读取消息\\n1.2.5\\u3000broker 和集群\\n一个独立的 Kafka 服务器被称为 broker。broker 接收来自生产者的消息，为消息设置偏移\\n量，并提交消息到磁盘保存。broker 为消费者提供服务\\n，对读取分区的请求作出响应，返\\n回已\\n经提交到磁盘上的消息。根据特定的硬件及其性能特征，单个 broker 可以轻松处 理数\\n千个分区以及每秒百万级的消息量。\\nbroker 是集群的组成部分。每个集群都有一个 broker 同时充当了集群控制器的角色（自动\\n从集群的活跃成员中选举出来） 。控制器负责管理工作，包括将分区分配给broker 和监控\\nbroker。在集群中，一个分区从属于一个 broker，该 broker 被称为分区的首领。一个分区\\n可以分配给多\\n个 broker，这个时候会发生 分区复制（见图 1-7） 。这种复制机制为分区提供\\n了消息冗余，如果有一个 broker 失效，其他 broker 可以接管领导权。不过，相关的消费者'), Document(id='caa9b6f1-d05c-4f45-af79-3cfe43d29d69', metadata={'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-03-22T07:33:14+00:00', 'source': '../data/Kafka权威指南-22-52.pdf', 'total_pages': 31, 'page': 6, 'page_label': '7', 'start_index': 0}, page_content='初识Kafka   ｜   7\\n生产者\\nBroker1\\nBroker2\\n主题A\\n的分区0\\n主题A\\n的分区1\\n主题A\\n的分区1\\n首领\\n首领\\n消息A/0 来自A/0的消息\\n来自A/1的消息消息A/1\\n复制A/0 复制A/1\\n主题A\\n的分区0\\n消费者\\nKafka集群\\n图 1-7：集群里的分区复制\\n保留消息（在一定期限内）是 Kafka 的一个重要特性。Kafka broker 默认的消息保留策略\\n是这样的：要么保留一段时间 （比如 7 天）\\n，要么保留到消息达到一定大小的字节数（比\\n如 1GB） 。当消息数量达到这些上限时\\n，旧消息就会过期并被删除，所以在任何时刻，可\\n用消息的总量都不会超过配置参数所指定的大小。主题可以配置自己的保留策略，可以将\\n消息保留到不再使用它们为止\\n。例如，用于跟踪用户活动的数据可能需要保留几天，而应\\n用程序的度量指标可能只需要保留几个小时。可以通过配置把主题当作 紧凑型日志，只有\\n最后一个带有特定键的消息会被保留下来。这种情况对于变更日志类型的数据来说比较适\\n用，因为人们只关心最后时刻发生的那个变更。\\n1.2.6\\u3000多集群\\n随着 Kafka 部署数量的增加，基于以下几点原因，最好使用多个集群。\\n• 数据类型分离\\n• 安全需求隔离\\n• 多数据中心（灾难恢复）\\n如果使用多个数据中心，就需要在它们之间复制消息。这样，在线应用程序才可以访问到\\n多个站点的用户活动信息。例如，如果一个用户修改了他们的资料信息，不管从哪个数据\\n中心都应该能看到这些改动。或者多个站点的监控数据可以被聚集到一个部署了分析程序\\n和告警系统的中心位置。不过，Kafka 的消息复制机制只能在单个集群里进行 ，不能在多\\n个集群之间进行。\\nKafka 提供了一个叫作 MirrorMaker 的工具，可以用它来实现集群间的消息复制 。\\nMirrorMaker 的核心组件包含了一个生产者和一个消费者\\n，两者之间通过一个队列相连。')]\n",
      "page_content='安装Kafka   ｜   25\n",
      "生产者\n",
      "生产者\n",
      "Kafka集群\n",
      "主题A\n",
      "主题A\n",
      "主题B\n",
      "分区0\n",
      "分区1\n",
      "分区0\n",
      "消费者\n",
      "图 2-2：一个简单的 Kafka 集群\n",
      "2.6.1　需要多少个broker\n",
      "一个 Kafka 集群需要多少个 broker 取决于以下几个因素。首先，需要多少磁盘空间来保\n",
      "留数据，以及单个 broker 有多少空间 可用。如果整个集群需要保留 10TB 的数据，每个\n",
      "broker 可以存储 2TB，那么至少需要 5 个 broker。如果启用了数据复制，那么至少还需要\n",
      "一倍的空间，不过这要取决于配置的复制系数是多少 （将在第 6 章介绍）\n",
      "。也就是说，如\n",
      "果启用了数据复制，那么这个集群至少需要 10 个 broker。\n",
      "第二\n",
      "个要考虑的因素是集群处理请求的能力。这通常与网络接口处理客户端流量的能力有\n",
      "关，特别是当有多个消费者存在或者在数据保留期间流量发生波动（比如高峰时段的流量\n",
      "爆发）时。如果单个 broker 的网络接口在 高峰时段可以达到 80% 的使用量，并且有两个\n",
      "消费者，那么消费者就无法保持峰值，除非有两个 broker。如果集群启用了复制功能，则\n",
      "要把这个额外的消费者考虑在内。因磁盘吞吐量低和系统内存不足造成的性能问题，也可\n",
      "以通过扩展多个 broker 来解决。\n",
      "2.6.2　broker 配置\n",
      "要把一个 broker 加入到集群里，只需要修改两个配置参数。首先，所有 broker 都必须配\n",
      "置相同的 zookeeper.connect，该参数指定了用于保存元数据的 Zookeeper 群组和路径。\n",
      "其次，每个 broker 都必须为 broker.id 参数设置唯一的值。如果两个 broker 使用相同的\n",
      "broker.id，那么第二个 broker 就无法启动。在运行集群时，还可以配置其他一些参数，特\n",
      "别是那些用于控制数据复制的参数，这些将在后续的章节介绍。' metadata={'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-03-22T07:33:14+00:00', 'source': '../data/Kafka权威指南-22-52.pdf', 'total_pages': 31, 'page': 24, 'page_label': '25', 'start_index': 0}\n",
      "Score: 0.7773890246747375\n",
      "\n",
      "page_content='安装Kafka   ｜   25\n",
      "生产者\n",
      "生产者\n",
      "Kafka集群\n",
      "主题A\n",
      "主题A\n",
      "主题B\n",
      "分区0\n",
      "分区1\n",
      "分区0\n",
      "消费者\n",
      "图 2-2：一个简单的 Kafka 集群\n",
      "2.6.1　需要多少个broker\n",
      "一个 Kafka 集群需要多少个 broker 取决于以下几个因素。首先，需要多少磁盘空间来保\n",
      "留数据，以及单个 broker 有多少空间 可用。如果整个集群需要保留 10TB 的数据，每个\n",
      "broker 可以存储 2TB，那么至少需要 5 个 broker。如果启用了数据复制，那么至少还需要\n",
      "一倍的空间，不过这要取决于配置的复制系数是多少 （将在第 6 章介绍）\n",
      "。也就是说，如\n",
      "果启用了数据复制，那么这个集群至少需要 10 个 broker。\n",
      "第二\n",
      "个要考虑的因素是集群处理请求的能力。这通常与网络接口处理客户端流量的能力有\n",
      "关，特别是当有多个消费者存在或者在数据保留期间流量发生波动（比如高峰时段的流量\n",
      "爆发）时。如果单个 broker 的网络接口在 高峰时段可以达到 80% 的使用量，并且有两个\n",
      "消费者，那么消费者就无法保持峰值，除非有两个 broker。如果集群启用了复制功能，则\n",
      "要把这个额外的消费者考虑在内。因磁盘吞吐量低和系统内存不足造成的性能问题，也可\n",
      "以通过扩展多个 broker 来解决。\n",
      "2.6.2　broker 配置\n",
      "要把一个 broker 加入到集群里，只需要修改两个配置参数。首先，所有 broker 都必须配\n",
      "置相同的 zookeeper.connect，该参数指定了用于保存元数据的 Zookeeper 群组和路径。\n",
      "其次，每个 broker 都必须为 broker.id 参数设置唯一的值。如果两个 broker 使用相同的\n",
      "broker.id，那么第二个 broker 就无法启动。在运行集群时，还可以配置其他一些参数，特\n",
      "别是那些用于控制数据复制的参数，这些将在后续的章节介绍。' metadata={'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-03-22T07:33:14+00:00', 'source': '../data/Kafka权威指南-22-52.pdf', 'total_pages': 31, 'page': 24, 'page_label': '25', 'start_index': 0}\n"
     ]
    }
   ],
   "source": [
    "# 向量存储 有多种方式可以内存，也可以用 FAISS 第三方向量库 这里方便起见 用内存\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "# 初始化向量存储\n",
    "vector_store = InMemoryVectorStore(embeddings)\n",
    "# 把文档做索引操作 加到库中\n",
    "ids = vector_store.add_documents(documents=all_splits)\n",
    "# 语义检索 返回语义相似的文档对象list 会自动对输入的文本进行嵌入\n",
    "results = vector_store.similarity_search(\n",
    "    \"Kafka Broker 配置\"\n",
    ")\n",
    "\n",
    "print(len(results))\n",
    "\n",
    "print(results)\n",
    "print(results[0])\n",
    "\n",
    "# 返回带分数的\n",
    "results = vector_store.similarity_search_with_score(\n",
    "    \"Kafka Broker 配置\"\n",
    ")\n",
    "doc, score = results[0]\n",
    "print(f\"Score: {score}\\n\")\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b99fff0b896fb05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "Score: 0.7773890246747375\n",
      "\n",
      "page_content='安装Kafka   ｜   25\n",
      "生产者\n",
      "生产者\n",
      "Kafka集群\n",
      "主题A\n",
      "主题A\n",
      "主题B\n",
      "分区0\n",
      "分区1\n",
      "分区0\n",
      "消费者\n",
      "图 2-2：一个简单的 Kafka 集群\n",
      "2.6.1　需要多少个broker\n",
      "一个 Kafka 集群需要多少个 broker 取决于以下几个因素。首先，需要多少磁盘空间来保\n",
      "留数据，以及单个 broker 有多少空间 可用。如果整个集群需要保留 10TB 的数据，每个\n",
      "broker 可以存储 2TB，那么至少需要 5 个 broker。如果启用了数据复制，那么至少还需要\n",
      "一倍的空间，不过这要取决于配置的复制系数是多少 （将在第 6 章介绍）\n",
      "。也就是说，如\n",
      "果启用了数据复制，那么这个集群至少需要 10 个 broker。\n",
      "第二\n",
      "个要考虑的因素是集群处理请求的能力。这通常与网络接口处理客户端流量的能力有\n",
      "关，特别是当有多个消费者存在或者在数据保留期间流量发生波动（比如高峰时段的流量\n",
      "爆发）时。如果单个 broker 的网络接口在 高峰时段可以达到 80% 的使用量，并且有两个\n",
      "消费者，那么消费者就无法保持峰值，除非有两个 broker。如果集群启用了复制功能，则\n",
      "要把这个额外的消费者考虑在内。因磁盘吞吐量低和系统内存不足造成的性能问题，也可\n",
      "以通过扩展多个 broker 来解决。\n",
      "2.6.2　broker 配置\n",
      "要把一个 broker 加入到集群里，只需要修改两个配置参数。首先，所有 broker 都必须配\n",
      "置相同的 zookeeper.connect，该参数指定了用于保存元数据的 Zookeeper 群组和路径。\n",
      "其次，每个 broker 都必须为 broker.id 参数设置唯一的值。如果两个 broker 使用相同的\n",
      "broker.id，那么第二个 broker 就无法启动。在运行集群时，还可以配置其他一些参数，特\n",
      "别是那些用于控制数据复制的参数，这些将在后续的章节介绍。' metadata={'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-03-22T07:33:14+00:00', 'source': '../data/Kafka权威指南-22-52.pdf', 'total_pages': 31, 'page': 24, 'page_label': '25', 'start_index': 0}\n"
     ]
    }
   ],
   "source": [
    "# 根据向量嵌入来查询相似性 和上面是一个意思 只不过这个是根据向量来查询，上面是自动嵌入\n",
    "query_embedding = embeddings.embed_query(\"Kafka Broker 配置\")\n",
    "results = vector_store.similarity_search_with_score_by_vector(query_embedding)\n",
    "\n",
    "print(len(results))\n",
    "doc, score = results[0]\n",
    "print(f\"Score: {score}\\n\")\n",
    "print(doc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
